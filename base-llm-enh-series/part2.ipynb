{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6419e0e9-4ca7-4fe9-8635-0b340e5faca9",
   "metadata": {},
   "source": [
    "## Full fine tuning of flan-T5 LLM model\n",
    "\n",
    "What is covered?\n",
    "1. Load flan-t5 model & dialogue-summarization dataset.\n",
    "2. Full fine-tune flan-T5 model on nvidia A6000 GPU\n",
    "3. Test inference of Base model and Fine-tuned model\n",
    "4. Test the fine-tuned model with rough and bleu scores\n",
    "5. Track the experiment with wandb (weights and biases)\n",
    "6. Learn to use Paperspace Gradient service to train your model for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2caf7dbd-d045-41a4-8931-f9f587cfbcd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T02:12:43.134211Z",
     "iopub.status.busy": "2023-12-21T02:12:43.132688Z",
     "iopub.status.idle": "2023-12-21T02:12:49.898186Z",
     "shell.execute_reply": "2023-12-21T02:12:49.897488Z",
     "shell.execute_reply.started": "2023-12-21T02:12:43.134180Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (23.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    wandb \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf56d83f-505f-4f61-9be7-736d0ac09aba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T02:12:49.899569Z",
     "iopub.status.busy": "2023-12-21T02:12:49.899399Z",
     "iopub.status.idle": "2023-12-21T02:12:55.370246Z",
     "shell.execute_reply": "2023-12-21T02:12:55.369150Z",
     "shell.execute_reply.started": "2023-12-21T02:12:49.899551Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maambekar234\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff9796-e7c9-4ed8-a648-be82ad9c670b",
   "metadata": {},
   "source": [
    "## 1. Load the flan-t5 model and dialogue summarization dataset\n",
    "1. check the datatype of model's tensor\n",
    "2. Check where exactly the model is loaded (cpu or gpu)\n",
    "3. Redo the datasplits for balalanced & optimum test/validation/test split\n",
    "4. Tokenize the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef3dc51-6c1d-4bcc-83eb-ef0a85cdd5d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T02:12:55.372452Z",
     "iopub.status.busy": "2023-12-21T02:12:55.371624Z",
     "iopub.status.idle": "2023-12-21T02:12:59.759692Z",
     "shell.execute_reply": "2023-12-21T02:12:59.758890Z",
     "shell.execute_reply.started": "2023-12-21T02:12:55.372430Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265e81bbf62e4c529372b05085371a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dialogue-summary dataset\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "#load model and tokenzier\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "# dtype check on model tensor. You could change it to bfloat16 to reduce the memory usage. \n",
    "# Note: bfloat16 won't work on Apple Silicon Macs\n",
    "dtype = next(original_model.parameters()).dtype\n",
    "print(f\"Tensor's dataType -->{dtype}\")\n",
    "\n",
    "#check where the model is loaded (should print either cpu or cuda)\n",
    "print(f\"Model is loaded on -->{next(original_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56959b7-2dea-4d36-9991-86ea7ae21c30",
   "metadata": {},
   "source": [
    "### 1.3 Redoing the datasplits for balalanced & optimum test/validation/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b45479-84fd-4dc0-b8bd-c46e5351dd7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T02:12:59.766782Z",
     "iopub.status.busy": "2023-12-21T02:12:59.766623Z",
     "iopub.status.idle": "2023-12-21T02:12:59.796549Z",
     "shell.execute_reply": "2023-12-21T02:12:59.795934Z",
     "shell.execute_reply.started": "2023-12-21T02:12:59.766769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-112982c8e6f56127.arrow\n"
     ]
    }
   ],
   "source": [
    "#redoing the dataset split as the default one is not balanced for model training\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "# Combine the splits (train, test, validation)\n",
    "combined_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "# Split the dataset into 80% train, 10% test, 10% validation\n",
    "train_test_split = combined_dataset.train_test_split(test_size=0.20)  # Splitting 20% for test+validation\n",
    "test_validation_split = train_test_split['test'].train_test_split(test_size=0.5)  # Splitting the 20% into two equal halves\n",
    "\n",
    "# Creating the final DatasetDict\n",
    "final_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': test_validation_split['test'],\n",
    "    'validation': test_validation_split['train']\n",
    "})\n",
    "\n",
    "test_summaries = final_dataset['test']['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e34ed-7187-4b2a-addb-8c74308f5c62",
   "metadata": {},
   "source": [
    "### 1.4 Tokenizing the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1f00f0-57f7-4f29-9b7e-180465ba2daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T02:12:59.801772Z",
     "iopub.status.busy": "2023-12-21T02:12:59.801627Z",
     "iopub.status.idle": "2023-12-21T02:13:08.369892Z",
     "shell.execute_reply": "2023-12-21T02:13:08.368969Z",
     "shell.execute_reply.started": "2023-12-21T02:12:59.801758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69704ec822e4ac59c8e20dc00c65661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11568 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11d09c4522a49e798a85205a4c0110d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1736 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab16f398d5641169f29056ae4717f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 11568\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1736\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1156\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompts = [start_prompt + dialogue + end_prompt for dialogue in examples[\"dialogue\"]]\n",
    "    model_max_input_length = tokenizer.model_max_length\n",
    "\n",
    "    # Tokenize the input dialogue text\n",
    "    tokenized_inputs = tokenizer(prompts, max_length=model_max_input_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Tokenize the labels for the dialogues\n",
    "    tokenized_labels = tokenizer(examples[\"summary\"], max_length=model_max_input_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # We need to replace the labels token ids of padding with -100 so they are not taken into account in the loss computation\n",
    "    tokenized_labels[\"input_ids\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels] for labels in tokenized_labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"labels\": tokenized_labels[\"input_ids\"]}\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_datasets = final_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove columns which are not necessary for training\n",
    "columns_to_remove = ['id', 'topic', 'dialogue', 'summary']\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c58ff4-79aa-4813-9627-185d550ac181",
   "metadata": {},
   "source": [
    "## 2 Full-finetune the flan-t5 model by training with above dataset & track experiment with wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b66cc7-eb07-40f8-a25f-c524071feca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T02:15:15.060133Z",
     "iopub.status.busy": "2023-12-21T02:15:15.059828Z",
     "iopub.status.idle": "2023-12-21T02:15:15.076104Z",
     "shell.execute_reply": "2023-12-21T02:15:15.075437Z",
     "shell.execute_reply.started": "2023-12-21T02:15:15.060115Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_rate = 3e-5\n",
    "wt_decay = 0.01\n",
    "early_st_th = 0.009 \n",
    "early_st_ptnce = 3\n",
    "steps = 250\n",
    "\n",
    "# wandb configuration for experiment tracking\n",
    "config={\n",
    "    'learning_rate': lr_rate,\n",
    "    'weight_decay': wt_decay,\n",
    "    'early_stopping_threshold' : early_st_th,\n",
    "    'early_stopping_patience':early_st_ptnce,\n",
    "    'steps':steps,\n",
    "    'per_device_train_batch_size':32,\n",
    "    'per_device_eval_batch_size':16,\n",
    "}\n",
    "\n",
    "timestamp = str(int(time.time()))\n",
    "\n",
    "output_dir = f'/notebooks/models/flant5-fullfinetuned-{timestamp}'\n",
    "\n",
    "# early stopping callback will help to stop the training if no siginficant reduction in error is observed.\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=early_st_ptnce, early_stopping_threshold=early_st_th)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"wandb\"\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=lr_rate,\n",
    "    auto_find_batch_size=True,\n",
    "    weight_decay=wt_decay,\n",
    "    logging_steps=steps,\n",
    "    eval_steps=steps,\n",
    "    max_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end = True,\n",
    "    gradient_accumulation_steps=2,   \n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=500, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model.to(\"cuda:0\"),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaff4df4-2185-4ebb-ac0e-a5006a41f015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T02:15:22.106818Z",
     "iopub.status.busy": "2023-12-21T02:15:22.106527Z",
     "iopub.status.idle": "2023-12-21T02:56:59.949961Z",
     "shell.execute_reply": "2023-12-21T02:56:59.948909Z",
     "shell.execute_reply.started": "2023-12-21T02:15:22.106800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:de8r9vhy) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">flant5-fullfinetune-1703124788</strong>: <a href=\"https://wandb.ai/aambekar234/genai-llm/runs/de8r9vhy\" target=\"_blank\">https://wandb.ai/aambekar234/genai-llm/runs/de8r9vhy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231221_021309-de8r9vhy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:de8r9vhy). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94bd2c4f8134f4a8c0e4ae5e913aa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669691300194245, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20231221_021522-3ip3crdy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aambekar234/genai-llm/runs/3ip3crdy\" target=\"_blank\">flant5-fullfinetune-1703124915</a></strong> to <a href=\"https://wandb.ai/aambekar234/genai-llm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2169' max='2169' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2169/2169 41:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.375600</td>\n",
       "      <td>1.176898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.278200</td>\n",
       "      <td>1.124501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.232800</td>\n",
       "      <td>1.091984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.179100</td>\n",
       "      <td>1.077777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.156200</td>\n",
       "      <td>1.064261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.153200</td>\n",
       "      <td>1.059030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.114900</td>\n",
       "      <td>1.058452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.116100</td>\n",
       "      <td>1.055144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='genai-llm', config=config, name=f'flant5-fullfinetune-{timestamp}')\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "run.log({\"Training time (seconds)\":training_time})\n",
    "run.log({\"Training configuration\":config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dfcc519-74cf-486a-b4e8-511741878efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T03:01:24.480972Z",
     "iopub.status.busy": "2023-12-21T03:01:24.480162Z",
     "iopub.status.idle": "2023-12-21T03:01:30.819351Z",
     "shell.execute_reply": "2023-12-21T03:01:30.818423Z",
     "shell.execute_reply.started": "2023-12-21T03:01:24.480948Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/notebooks/models/flant5-fullfinetuned-1703124915/final)... Done. 4.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x7f26d0599490>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the best model and tokenizer\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final\")\n",
    "\n",
    "model_artifact = wandb.Artifact('model_artifact', type='model')\n",
    "model_artifact.add_dir(f\"{output_dir}/final\")\n",
    "run.log_artifact(model_artifact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6be4c-95f4-4ee9-b414-0b1cbedbd547",
   "metadata": {},
   "source": [
    "## 3. Now let's compare the inference of the original and the fine-tuned model with zero shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7432ba8-1522-4f1f-aac1-60dd37d044b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T03:08:09.185651Z",
     "iopub.status.busy": "2023-12-21T03:08:09.185298Z",
     "iopub.status.idle": "2023-12-21T03:08:13.095018Z",
     "shell.execute_reply": "2023-12-21T03:08:13.094371Z",
     "shell.execute_reply.started": "2023-12-21T03:08:09.185627Z"
    }
   },
   "outputs": [],
   "source": [
    "## load the new model and tokenizer\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(f\"{output_dir}/final\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(f\"{output_dir}/final\")\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a4a563-e7bc-497d-bf3a-162fcba301f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T03:08:17.471561Z",
     "iopub.status.busy": "2023-12-21T03:08:17.471245Z",
     "iopub.status.idle": "2023-12-21T03:08:18.937641Z",
     "shell.execute_reply": "2023-12-21T03:08:18.936737Z",
     "shell.execute_reply.started": "2023-12-21T03:08:17.471543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "Summary-->\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "#let's get inference from original model\n",
    "example_record = 200\n",
    "dialogue = dataset['test'][example_record]['dialogue']\n",
    "\n",
    "print(dialogue)\n",
    "\n",
    "start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "end_prompt = '\\n\\nSummary: '\n",
    "prompt = start_prompt + dialogue + end_prompt\n",
    "\n",
    "\n",
    "input = tokenizer(prompt, return_tensors='pt')\n",
    "output_tokens = original_model.generate(input[\"input_ids\"], max_new_tokens=50,)\n",
    "original_model_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary-->\")\n",
    "print(original_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ad3c3d4-c541-43b1-b808-7531a74261c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T03:08:29.825428Z",
     "iopub.status.busy": "2023-12-21T03:08:29.824144Z",
     "iopub.status.idle": "2023-12-21T03:08:56.233531Z",
     "shell.execute_reply": "2023-12-21T03:08:56.232668Z",
     "shell.execute_reply.started": "2023-12-21T03:08:29.825403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Human Baseline Summary -->\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "#### Summary Generated by original model->\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "#### Summary Generated by finetuned model->\n",
      "#Person2# wants to upgrade #Person2#'s system and hardware. #Person1# suggests adding a painting program to #Person2#'s software and adding a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "#lets get inference from finetuned model\n",
    "\n",
    "input = tokenizer2(prompt, return_tensors='pt')\n",
    "output_tokens = finetuned_model.generate(input[\"input_ids\"], max_new_tokens=50,)\n",
    "finetuned_model_output = tokenizer2.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"#### Human Baseline Summary -->\")\n",
    "print(dataset['test'][example_record]['summary'])\n",
    "print(\"#### Summary Generated by original model->\")\n",
    "print(original_model_output)\n",
    "print(\"#### Summary Generated by finetuned model->\")\n",
    "print(finetuned_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9573e90-0c3f-4741-b1a3-c8a1ff95975e",
   "metadata": {},
   "source": [
    "### Now lets Evaluate the model with ROUGE & BLEU Score & compare them with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa958021-8212-4b31-ba7d-5d74c71a4aa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T03:10:25.824361Z",
     "iopub.status.busy": "2023-12-21T03:10:25.823245Z",
     "iopub.status.idle": "2023-12-21T03:12:58.080207Z",
     "shell.execute_reply": "2023-12-21T03:12:58.079074Z",
     "shell.execute_reply.started": "2023-12-21T03:10:25.824335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries from original & finetuned models...:  55%|█████▍    | 82/150 [01:24<01:12,  1.06s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Generating summaries from original & finetuned models...: 100%|██████████| 150/150 [02:31<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# to save time we will only use 150 items from test split for evaluation\n",
    "dialogues = final_dataset['test']['dialogue'][:150]\n",
    "print(len(dialogues))\n",
    "\n",
    "human_baseline_summaries = final_dataset['test']['dialogue'][:150]\n",
    "original_model_summaries = []\n",
    "finetuned_model_summaries = []\n",
    "\n",
    "# moving both models to gpu for faster inference\n",
    "original_model.to(\"cuda:0\")\n",
    "finetuned_model.to(\"cuda:0\")\n",
    "\n",
    "for dialogue in tqdm(dialogues, desc=\"Generating summaries from original & finetuned models...\"):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following conversation.\n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    finetuned_model_outputs = finetuned_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    finetuned_model_text_output = tokenizer.decode(finetuned_model_outputs[0], skip_special_tokens=True)\n",
    "    finetuned_model_summaries.append(finetuned_model_text_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3828ad4-ed0c-4420-8ba7-f0fb3c1d72e1",
   "metadata": {},
   "source": [
    "### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f6e756f-4b9f-452d-9f9c-3a44debdadf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T03:14:29.660153Z",
     "iopub.status.busy": "2023-12-21T03:14:29.658645Z",
     "iopub.status.idle": "2023-12-21T03:14:30.820851Z",
     "shell.execute_reply": "2023-12-21T03:14:30.819762Z",
     "shell.execute_reply.started": "2023-12-21T03:14:29.660122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e618f3bcfa74f01940bfb6a74e9d8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.23894258937571508, 'rouge2': 0.08332541521688881, 'rougeL': 0.2055799206592445, 'rougeLsum': 0.20576620785444855}\n",
      "Finetuned MODEL:\n",
      "{'rouge1': 0.48914574569513863, 'rouge2': 0.2351370231982014, 'rougeL': 0.40066456430517255, 'rougeLsum': 0.3989172126204001}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "human_baseline_summaries = test_summaries[:150]\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "finetuned_model_results = rouge.compute(\n",
    "    predictions=finetuned_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(finetuned_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('Finetuned MODEL:')\n",
    "print(finetuned_model_results)\n",
    "\n",
    "run.log({\"rouge_score\": finetuned_model_results})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77306bc7-8adc-435e-9bfc-15275bde4076",
   "metadata": {},
   "source": [
    "### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bcddd30-ffb5-4829-8e38-d7f17420c0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-21T03:14:43.407875Z",
     "iopub.status.busy": "2023-12-21T03:14:43.407519Z",
     "iopub.status.idle": "2023-12-21T03:14:50.365393Z",
     "shell.execute_reply": "2023-12-21T03:14:50.364389Z",
     "shell.execute_reply.started": "2023-12-21T03:14:43.407854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1131d888c145e0a88daa85b58606ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f329bb6153d411db05c3523862e0431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8d5c870eb74ce5b526d797cb195789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'bleu': 0.06830401333488964, 'precisions': [0.25926829268292684, 0.11721518987341772, 0.05973684210526316, 0.019452054794520546], 'brevity_penalty': 0.8860555704408019, 'length_ratio': 0.8920800696257616, 'translation_length': 4100, 'reference_length': 4596}\n",
      "Finetuned MODEL:\n",
      "{'bleu': 0.23373348653030568, 'precisions': [0.4885386819484241, 0.2929701877070298, 0.18981831945495836, 0.1098558628749513], 'brevity_penalty': 1.0, 'length_ratio': 1.2149695387293298, 'translation_length': 5584, 'reference_length': 4596}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training time (seconds)</td><td>▁</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▁█▄▄▆▅▄</td></tr><tr><td>eval/samples_per_second</td><td>▅█▁▅▅▃▄▅</td></tr><tr><td>eval/steps_per_second</td><td>▅█▁▅▅▃▄▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▆▆▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▆▆▇▇█████</td></tr><tr><td>train/learning_rate</td><td>▄█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training time (seconds)</td><td>2491.59128</td></tr><tr><td>eval/loss</td><td>1.05514</td></tr><tr><td>eval/runtime</td><td>23.0015</td></tr><tr><td>eval/samples_per_second</td><td>50.258</td></tr><tr><td>eval/steps_per_second</td><td>6.304</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>2169</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1161</td></tr><tr><td>train/total_flos</td><td>2.376381915935539e+16</td></tr><tr><td>train/train_loss</td><td>1.19512</td></tr><tr><td>train/train_runtime</td><td>2491.1806</td></tr><tr><td>train/train_samples_per_second</td><td>13.931</td></tr><tr><td>train/train_steps_per_second</td><td>0.871</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">flant5-fullfinetune-1703124915</strong>: <a href=\"https://wandb.ai/aambekar234/genai-llm/runs/3ip3crdy\" target=\"_blank\">https://wandb.ai/aambekar234/genai-llm/runs/3ip3crdy</a><br/>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231221_021522-3ip3crdy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "    \n",
    "original_model_results = bleu.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries\n",
    ")\n",
    "\n",
    "finetuned_model_results = bleu.compute(\n",
    "    predictions=finetuned_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('Finetuned MODEL:')\n",
    "print(finetuned_model_results)\n",
    "\n",
    "run.log({\"bleu_score\": finetuned_model_results})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3981947-77e1-408b-845f-4e734d50d643",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we can see that with full-finetuning we managed to get great summaries without employing few-shot learning. As this process is very resource intensive we will explore much more efficient technique called LoRA in next article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17516159-9cd7-455d-bb44-ad91f74fd0d9",
   "metadata": {},
   "source": [
    "**🌟 Connect on LinkedIn!** \n",
    "\n",
    "If you've found this content _useful_ and would like to explore more about **data science**, **machine learning**, and related fields, I'd be delighted to see you on my LinkedIn network. I share insights, resources, and the latest trends that could be beneficial for your learning journey.\n",
    "\n",
    "➤ [**_Follow on LinkedIn_**](https://www.linkedin.com/in/aambekar234/)\n",
    "\n",
    "_Your support and interaction are always appreciated._\n",
    "\n",
    "**Best Regards,**\n",
    "**Abhijeet Ambekar**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
