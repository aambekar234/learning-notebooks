{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0eae2a2-44f5-4214-9435-74ef068d1ab0",
   "metadata": {},
   "source": [
    "## GenAI - Summarize the text with Base LLMS and employ various techniques to improve it. \n",
    "\n",
    "In this notebook, we will learn how to use Hugging Face hosted Base LLMS to summarize a dialogue. We will see that the base LLMs do not perform very well in the summarization task but they can be improved with writing prompts and in-context learning (few shot learning). We will go over below topics.  \n",
    "1. Use FLAN-T5 model to summarize dialogues.\n",
    "2. Use Prompt to improve the inference output.\n",
    "3. Use In-context learning and different configuration to get the best results for summarization inference on the same base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584012ea-8165-4de5-90a2-a8a7666c7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a035a4-451d-43c0-a1ec-b9780e8cdccf",
   "metadata": {},
   "source": [
    "### 1. Lets Summarize Dialogue without prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc93b3e-454d-446a-bb53-13379eedc9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dialogue': '#Person1#: Have you considered upgrading your system?\\n'\n",
      "             \"#Person2#: Yes, but I'm not sure what exactly I would need.\\n\"\n",
      "             '#Person1#: You could consider adding a painting program to your '\n",
      "             'software. It would allow you to make up your own flyers and '\n",
      "             'banners for advertising.\\n'\n",
      "             '#Person2#: That would be a definite bonus.\\n'\n",
      "             '#Person1#: You might also want to upgrade your hardware because '\n",
      "             'it is pretty outdated now.\\n'\n",
      "             '#Person2#: How can we do that?\\n'\n",
      "             \"#Person1#: You'd probably need a faster processor, to begin \"\n",
      "             'with. And you also need a more powerful hard disc, more memory '\n",
      "             'and a faster modem. Do you have a CD-ROM drive?\\n'\n",
      "             '#Person2#: No.\\n'\n",
      "             '#Person1#: Then you might want to add a CD-ROM drive too, '\n",
      "             'because most new software programs are coming out on Cds.\\n'\n",
      "             '#Person2#: That sounds great. Thanks.',\n",
      " 'id': 'test_66_3',\n",
      " 'summary': '#Person1# teaches #Person2# how to upgrade software and hardware '\n",
      "            \"in #Person2#'s system.\",\n",
      " 'topic': 'upgrading system'}\n"
     ]
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "example_record = 200\n",
    "pprint(dataset['test'][example_record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbc8f4c-f3f2-4f31-8172-eab1cb70c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load google flan-t5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915bc7c0-c156-498a-b66e-4b7de5946d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Human Baseline Summary -->\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "#### Base Model Inference Summary -->\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "# summarize the text with base LLM and compare it with human baseline.\n",
    "dialogue = dataset['test'][example_record]['dialogue']\n",
    "inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "output_tokens = model.generate(inputs[\"input_ids\"], max_new_tokens=50,)\n",
    "model_inference_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "print(\"#### Human Baseline Summary -->\")\n",
    "print(dataset['test'][example_record]['summary'])\n",
    "print(\"#### Base Model Inference Summary -->\")\n",
    "print(model_inference_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1fe1bd-48a7-43d8-a9bc-fe4e37a31b15",
   "metadata": {},
   "source": [
    "#### We can see that the model poorly in inferring the summary. \n",
    "\n",
    "### 2. Lets use prompt engineering now and try to get better results (This method is also called zero-shot instruction prompt.)\n",
    "We will try to use below two prompts. \n",
    "1. Generic instruction prompt\n",
    "2. FLAN-T5 template prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d60e3ef0-1fc4-400a-ab4e-77806fdbdbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Human Baseline Summary -->\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "#### Summary Generated by generic prompt->\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "#### Summary Generated by template prompt->\n",
      "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n"
     ]
    }
   ],
   "source": [
    "generic_prompt = f\"\"\"\n",
    "Summarize the following conversation. \n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "template_prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "inputs = tokenizer(generic_prompt, return_tensors='pt')\n",
    "output_tokens = model.generate(inputs[\"input_ids\"], max_new_tokens=50,)\n",
    "generic_prompt_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer(template_prompt, return_tensors='pt')\n",
    "output_tokens = model.generate(inputs[\"input_ids\"], max_new_tokens=50,)\n",
    "template_prompt_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"#### Human Baseline Summary -->\")\n",
    "print(dataset['test'][example_record]['summary'])\n",
    "print(\"#### Summary Generated by generic prompt->\")\n",
    "print(generic_prompt_output)\n",
    "print(\"#### Summary Generated by template prompt->\")\n",
    "print(template_prompt_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79b7df-3895-4692-9693-6fb932397d40",
   "metadata": {},
   "source": [
    "#### We can see that even prompt engineering did not generate helpful summary with the base model. \n",
    "### 3. Let's try one-shot or few-shot inference now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9562fdea-3ab5-4074-a986-1c6b34c707aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which generates a few shot prompt. \n",
    "def generate_prompt(examples_indices, input_index):\n",
    "    \"\"\"\n",
    "    Generates a few-shot learning prompt using specified examples from dataset object.\n",
    "\n",
    "    :param examples_indices: List of indices for examples in the dataset to be used in the prompt.\n",
    "    :param input_index: Index of the input dialogue in the dataset for which the summary is to be generated.\n",
    "    :return: A string prompt for a few-shot learning.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "\n",
    "    # Adding few-shot examples\n",
    "    for idx in examples_indices:\n",
    "        example = dataset['test'][idx]\n",
    "        prompt += f\"\\nDialogue:\\n\\n{example['dialogue']}\\n\\nWhat was going on?\\n\\n{example['summary']}\\n\\n\"\n",
    "\n",
    "    # Adding the input dialogue\n",
    "    input_dialogue = dataset['test'][input_index]['dialogue']\n",
    "    prompt += f\"\\nDialogue:\\n\\n{input_dialogue}\\n\\nWhat was going on?\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "705c932c-b92f-417d-8872-f0a0a5236cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: John dates her seven times a week.\n",
      "#Person2#: Really? That's a straws in the wind.\n",
      "#Person1#: I think so. Maybe he's fallen for her.\n",
      "#Person2#: Yeah. They suit each other. A perfect match between a man and a girl.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "#Person1# and #Person2# are talking about a couple.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n",
      "#Person1# is giving #Person2# some advice for upgrading #Person2#'s system, such as adding a painting program and a faster processor.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the few shot prompt, here we are passing examples 197 & 198 as in-context knowledge\n",
    "prompt = generate_prompt([197, 198], 200)\n",
    "#lets print the few shot prompt we generated\n",
    "print(f\"{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8eb028c-766c-465f-9d5a-9024272ff5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1 is giving #Person2# some advice for upgrading #Person2#'s system, such as adding a painting program and a faster processor.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get results by the above few shot prompt\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "# we have passed temperature variable in generation config here. The variable controls the creativity of the model.\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.2)\n",
    "output_tokens = model.generate(inputs[\"input_ids\"],generation_config=generation_config,)\n",
    "few_shot_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "print(few_shot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dc8cae6-9516-4c44-9403-45adb1b932e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Human Baseline Summary -->\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "#### Base Model Inference Summary -->\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "#### Summary Generated by generic prompt -->\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "#### Summary Generated by template prompt -->\n",
      "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
      "#### Summary Generated by few-shot learning -->\n",
      "#Person1 is giving #Person2# some advice for upgrading #Person2#'s system, such as adding a painting program and a faster processor.\n"
     ]
    }
   ],
   "source": [
    "print(\"#### Human Baseline Summary -->\")\n",
    "print(dataset['test'][example_record]['summary'])\n",
    "print(\"#### Base Model Inference Summary -->\")\n",
    "print(model_inference_output)\n",
    "print(\"#### Summary Generated by generic prompt -->\")\n",
    "print(generic_prompt_output)\n",
    "print(\"#### Summary Generated by template prompt -->\")\n",
    "print(template_prompt_output)\n",
    "print(\"#### Summary Generated by few-shot learning -->\")\n",
    "print(few_shot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a6075-bf76-4bd2-a0bc-a3ffa620daf2",
   "metadata": {},
   "source": [
    "### Results and whats next?\n",
    "As we can see that few shot/in-context learning improves the output generation of base model significantly. It was observed that the helpfulness of in-context examples don't add much value after 3-4 examples. To overcome this, we can use model fine tuning techniques to achieve better results. Stay tuned for the next article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b500038f-73d8-4204-aa53-a914aaa68821",
   "metadata": {},
   "source": [
    "➤ [**Connect on LinkedIn**](https://www.linkedin.com/in/aambekar234/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfbbee-5b82-4665-8f07-9323406282c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
