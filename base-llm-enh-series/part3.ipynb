{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e57fe8e-8ae1-4533-af2a-75caff1692a1",
   "metadata": {},
   "source": [
    "## PEFT (Parameter Efficient Fine-Tuning) flan-T5 LLM model\n",
    "\n",
    "What is covered?\n",
    "1. Load flan-t5 model & dialogue-summarization dataset.\n",
    "2. Paramerter Efficient fine-tuning of flan-T5 model on nvidia A6000 GPU\n",
    "3. Test inference of Base model and Fine-tuned model\n",
    "4. Test the fine-tuned model with rough and bleu scores\n",
    "5. Track the experiment with wandb (weights and biases)\n",
    "6. Learn to use Paperspace Gradient service to train your model for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7dd752-ced1-45f8-a8df-ad86811fed57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T14:15:43.991971Z",
     "iopub.status.busy": "2023-12-26T14:15:43.991442Z",
     "iopub.status.idle": "2023-12-26T14:16:45.527653Z",
     "shell.execute_reply": "2023-12-26T14:16:45.527141Z",
     "shell.execute_reply.started": "2023-12-26T14:15:43.991949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20ecdbd-bbab-49f1-aeba-5289e1942a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T14:18:24.091678Z",
     "iopub.status.busy": "2023-12-26T14:18:24.091393Z",
     "iopub.status.idle": "2023-12-26T14:18:40.224482Z",
     "shell.execute_reply": "2023-12-26T14:18:40.223729Z",
     "shell.execute_reply.started": "2023-12-26T14:18:24.091656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import statements\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366def5-f8f3-4442-a09f-30b12828550a",
   "metadata": {},
   "source": [
    "## 1. Load the flan-t5 model and dialogue summarization dataset\n",
    "1. Check the datatype of model's tensor\n",
    "2. Check where exactly the model is loaded (cpu or gpu)\n",
    "3. Redo the datasplits for balalanced & optimum test/validation/test split\n",
    "4. Tokenize the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc5bd88-1ebe-4193-a7e8-b1b909197419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T14:18:46.713370Z",
     "iopub.status.busy": "2023-12-26T14:18:46.711857Z",
     "iopub.status.idle": "2023-12-26T14:19:14.414184Z",
     "shell.execute_reply": "2023-12-26T14:19:14.413539Z",
     "shell.execute_reply.started": "2023-12-26T14:18:46.713327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f863e3d933b94806bd10095c7ef16a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e11641da644cca84f79f0b60757c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8a86fac95a41358b6af45006d96db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b4a2caaa6946738ed017ebf5882c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52584f18a07e49bf9a4fac7fd8de02ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a6e3315d4c4339b7338841980530ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62323124dd894bc487bfd13a083bf010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c6ca27fd86468d99c076f9b916e52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989d922c539d4decbcef9d7a4e6cdb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f118bfc4b6b542289d1ce1e28f9d5c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1533464e2f4c8db57f2d8a217174dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"config.json\";:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a22e3f5ff34e80a6f51130cf3fb9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"model.safetensors\";:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90d6a6b9d6042f5b10bc2e43855dd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)ration_config.json\";:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3064dd0648de4e3b9aaf5062ac491953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)enizer_config.json\";:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482d62ac532145c084a6ed883f35fdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"spiece.model\";:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5fed49da254586a0e042865fa7a334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"tokenizer.json\";:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208bd998fa4b4980bd49d7740c2a161d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)al_tokens_map.json\";:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor's dataType -->torch.float32\n",
      "Model is loaded on -->cpu\n"
     ]
    }
   ],
   "source": [
    "# load dialogue-summary dataset\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "#load model and tokenzier\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "# dtype check on model tensor. You could change it to bfloat16 to reduce the memory usage. \n",
    "# Note: bfloat16 won't work on Apple Silicon Macs\n",
    "dtype = next(original_model.parameters()).dtype\n",
    "print(f\"Tensor's dataType -->{dtype}\")\n",
    "\n",
    "#check where the model is loaded (should print either cpu or cuda)\n",
    "print(f\"Model is loaded on -->{next(original_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6429624-f413-4c57-95d1-7274b5cc1147",
   "metadata": {},
   "source": [
    "### 1.3 Redoing the datasplits for balalanced & optimum test/validation/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e6fa512-3c30-4255-9a6c-5185e98dfb98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T14:19:14.415467Z",
     "iopub.status.busy": "2023-12-26T14:19:14.415283Z",
     "iopub.status.idle": "2023-12-26T14:19:14.445881Z",
     "shell.execute_reply": "2023-12-26T14:19:14.445314Z",
     "shell.execute_reply.started": "2023-12-26T14:19:14.415450Z"
    }
   },
   "outputs": [],
   "source": [
    "#redoing the dataset split as the default one is not balanced for model training\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "# Combine the splits (train, test, validation)\n",
    "combined_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "# Split the dataset into 80% train, 10% test, 10% validation\n",
    "train_test_split = combined_dataset.train_test_split(test_size=0.20)  # Splitting 20% for test+validation\n",
    "test_validation_split = train_test_split['test'].train_test_split(test_size=0.5)  # Splitting the 20% into two equal halves\n",
    "\n",
    "# Creating the final DatasetDict\n",
    "final_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': test_validation_split['test'],\n",
    "    'validation': test_validation_split['train']\n",
    "})\n",
    "\n",
    "test_summaries = final_dataset['test']['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4af25-b679-4daa-9667-ce71233a6f41",
   "metadata": {},
   "source": [
    "### 1.4 Tokenizing the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a201e7de-724c-4cf3-88a2-1c60501fc4d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T14:19:14.446867Z",
     "iopub.status.busy": "2023-12-26T14:19:14.446718Z",
     "iopub.status.idle": "2023-12-26T14:19:23.293941Z",
     "shell.execute_reply": "2023-12-26T14:19:23.293523Z",
     "shell.execute_reply.started": "2023-12-26T14:19:14.446851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597c199298444208b684a8571f7e18bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11568 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d6eb848c504225a238a7d53ecab496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c28e14c9e94e548df7776beb616265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompts = [start_prompt + dialogue + end_prompt for dialogue in examples[\"dialogue\"]]\n",
    "    model_max_input_length = tokenizer.model_max_length\n",
    "\n",
    "    # Tokenize the input dialogue text\n",
    "    tokenized_inputs = tokenizer(prompts, max_length=model_max_input_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Tokenize the labels for the dialogues\n",
    "    tokenized_labels = tokenizer(examples[\"summary\"], max_length=model_max_input_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # We need to replace the labels token ids of padding with -100 so they are not taken into account in the loss computation\n",
    "    tokenized_labels[\"input_ids\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels] for labels in tokenized_labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"labels\": tokenized_labels[\"input_ids\"]}\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_datasets = final_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove columns which are not necessary for training\n",
    "columns_to_remove = ['id', 'topic', 'dialogue', 'summary']\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e2570-8225-4b4d-9520-0bb892752d9a",
   "metadata": {},
   "source": [
    "## 2. Perfrom Parameter Efficient Fine Tuning (PEFT)\n",
    "\n",
    "PEFT, encompassing Low-Rank Adaptation (LoRA) and prompt tuning (distinct from prompt engineering), is primarily associated with LoRA. LoRA enables efficient model fine-tuning, often utilizing minimal computational resources such as a single GPU. The process retains the original Large Language Model (LLM) intact while generating a significantly smaller \"LoRA adapter,\" typically a single-digit percentage of the original LLM's size (measured in MBs compared to GBs). For inference, this adapter is integrated with the original LLM. The advantage of LoRA lies in its ability to use multiple adapters with a single LLM, optimizing memory usage across various tasks and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7dbc4a1-9bbb-4e0d-809a-94852c143db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T14:24:47.206898Z",
     "iopub.status.busy": "2023-12-26T14:24:47.206312Z",
     "iopub.status.idle": "2023-12-26T14:24:47.665024Z",
     "shell.execute_reply": "2023-12-26T14:24:47.664466Z",
     "shell.execute_reply.started": "2023-12-26T14:24:47.206870Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"], # focusing on query and value \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "peft_model = get_peft_model(original_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41497a2-f1ce-4969-9c9e-517e5d715ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T14:24:55.676634Z",
     "iopub.status.busy": "2023-12-26T14:24:55.675985Z",
     "iopub.status.idle": "2023-12-26T14:24:57.167278Z",
     "shell.execute_reply": "2023-12-26T14:24:57.166681Z",
     "shell.execute_reply.started": "2023-12-26T14:24:55.676610Z"
    }
   },
   "outputs": [],
   "source": [
    "timestamp = str(int(time.time()))\n",
    "\n",
    "output_dir = f'./peft-models/peft-dialogue-summary-training-{timestamp}'\n",
    "\n",
    "# early stopping callback will help to stop the training if no siginficant reduction in error is observed.\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.009)\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-4, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    max_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end = True,\n",
    "    gradient_accumulation_steps=2,   \n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=250, \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "266c5e12-8d0b-4ce1-8131-4c8e446b52ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T14:25:33.448523Z",
     "iopub.status.busy": "2023-12-26T14:25:33.447804Z",
     "iopub.status.idle": "2023-12-26T14:50:34.533774Z",
     "shell.execute_reply": "2023-12-26T14:50:34.533261Z",
     "shell.execute_reply.started": "2023-12-26T14:25:33.448488Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maambekar234\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20231226_142533-v3u0q5xn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aambekar234/genai-llm/runs/v3u0q5xn\" target=\"_blank\">flant5-PEFT-1703600695</a></strong> to <a href=\"https://wandb.ai/aambekar234/genai-llm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 24:57, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.098500</td>\n",
       "      <td>1.487497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.555300</td>\n",
       "      <td>1.255934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.363500</td>\n",
       "      <td>1.203612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.316400</td>\n",
       "      <td>1.187709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.331000</td>\n",
       "      <td>1.177025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.301700</td>\n",
       "      <td>1.162241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.278400</td>\n",
       "      <td>1.161294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.280700</td>\n",
       "      <td>1.153541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.264200</td>\n",
       "      <td>1.152996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.285700</td>\n",
       "      <td>1.151971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./peft-models/peft-dialogue-summary-training-1703600695/final/tokenizer_config.json',\n",
       " './peft-models/peft-dialogue-summary-training-1703600695/final/special_tokens_map.json',\n",
       " './peft-models/peft-dialogue-summary-training-1703600695/final/spiece.model',\n",
       " './peft-models/peft-dialogue-summary-training-1703600695/final/added_tokens.json',\n",
       " './peft-models/peft-dialogue-summary-training-1703600695/final/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = wandb.init(project='genai-llm', name=f'flant5-PEFT-{timestamp}')\n",
    "\n",
    "start_time = time.time()\n",
    "peft_trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "run.log({\"Training time (seconds)\":training_time})\n",
    "run.log({\"Training configuration\":peft_training_args.to_dict()})\n",
    "\n",
    "peft_model_path=f\"{output_dir}/final/\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22af273c-9901-42fd-a664-049eb1ebe3ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T15:52:50.856674Z",
     "iopub.status.busy": "2023-12-26T15:52:50.856348Z",
     "iopub.status.idle": "2023-12-26T15:52:53.516555Z",
     "shell.execute_reply": "2023-12-26T15:52:53.515990Z",
     "shell.execute_reply.started": "2023-12-26T15:52:50.856651Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       peft_model_path,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbb8c555-f497-4d4a-8ceb-167b3209b4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T15:53:07.695030Z",
     "iopub.status.busy": "2023-12-26T15:53:07.694279Z",
     "iopub.status.idle": "2023-12-26T15:53:10.892997Z",
     "shell.execute_reply": "2023-12-26T15:53:10.892583Z",
     "shell.execute_reply.started": "2023-12-26T15:53:07.695018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "Summary-->\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "#let's get inference from original model\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "example_record = 200\n",
    "dialogue = dataset['test'][example_record]['dialogue']\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "print(dialogue)\n",
    "\n",
    "start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "end_prompt = '\\n\\nSummary: '\n",
    "prompt = start_prompt + dialogue + end_prompt\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "output_tokens = original_model.generate(input_ids=input_ids, generation_config = generation_config,)\n",
    "original_model_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary-->\")\n",
    "print(original_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d7f42d2-bed2-4fd3-a527-4ad5dfdcc82b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T15:59:31.470867Z",
     "iopub.status.busy": "2023-12-26T15:59:31.470244Z",
     "iopub.status.idle": "2023-12-26T15:59:33.963909Z",
     "shell.execute_reply": "2023-12-26T15:59:33.963304Z",
     "shell.execute_reply.started": "2023-12-26T15:59:31.470841Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model_artifact:v0, 947.60MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:0.0\n"
     ]
    }
   ],
   "source": [
    "# load full fine tuned model we trained in part2\n",
    "artifact = run.use_artifact('aambekar234/genai-llm/model_artifact:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "#let's get inference from original model\n",
    "fullfinetuned_model = AutoModelForSeq2SeqLM.from_pretrained(artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f4cfefc-40d3-4438-9e8a-a20509bc8921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T16:01:24.617253Z",
     "iopub.status.busy": "2023-12-26T16:01:24.616639Z",
     "iopub.status.idle": "2023-12-26T16:01:26.074058Z",
     "shell.execute_reply": "2023-12-26T16:01:26.073499Z",
     "shell.execute_reply.started": "2023-12-26T16:01:24.617228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Human Baseline Summary -->\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "#### Summary Generated by original model->\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "#### Summary Generated by finetuned model->\n",
      "#Person2# wants to upgrade #Person2#'s system and hardware. #Person1# suggests adding a painting program to #Person2#'s software and adding a CD-ROM drive.\n",
      "#### Summary Generated by peft model->\n",
      "#Person2# considers upgrading #Person1#'s system and hardware. #Person1# recommends adding a painting program to #Person2#'s software. #Person2# also considers adding a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "#lets get inference from peft model\n",
    "fullfinetuned_model.to(\"cuda:0\")\n",
    "output_tokens = fullfinetuned_model.generate(input_ids=input_ids.to(\"cuda:0\"), generation_config = GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "finetuned_model_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "#lets get inference from peft model\n",
    "peft_model.to(\"cuda:0\")\n",
    "output_tokens = peft_model.generate(input_ids=input_ids.to(\"cuda:0\"), generation_config = GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"#### Human Baseline Summary -->\")\n",
    "print(dataset['test'][example_record]['summary'])\n",
    "print(\"#### Summary Generated by original model->\")\n",
    "print(original_model_output)\n",
    "print(\"#### Summary Generated by finetuned model->\")\n",
    "print(finetuned_model_output)\n",
    "print(\"#### Summary Generated by peft model->\")\n",
    "print(peft_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e98048ab-4344-4d45-9844-7c8634c3f41b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T16:07:01.875854Z",
     "iopub.status.busy": "2023-12-26T16:07:01.875219Z",
     "iopub.status.idle": "2023-12-26T16:10:38.375374Z",
     "shell.execute_reply": "2023-12-26T16:10:38.374817Z",
     "shell.execute_reply.started": "2023-12-26T16:07:01.875839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries from original & finetuned models...:  31%|â–ˆâ–ˆâ–ˆ       | 46/150 [01:04<02:37,  1.51s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Generating summaries from original & finetuned models...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [03:36<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# to save time we will only use 150 items from test split for evaluation\n",
    "dialogues = final_dataset['test']['dialogue'][:150]\n",
    "print(len(dialogues))\n",
    "\n",
    "human_baseline_summaries = final_dataset['test']['dialogue'][:150]\n",
    "original_model_summaries = []\n",
    "fullfinetuned_model_smmaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "# moving model to gpu for faster inference\n",
    "original_model.to(\"cuda:0\")\n",
    "\n",
    "for dialogue in tqdm(dialogues, desc=\"Generating summaries from original & finetuned models...\"):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following conversation.\n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config = generation_config)\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config = generation_config)\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "    \n",
    "    fullfinetuned_model_outputs = fullfinetuned_model.generate(input_ids=input_ids, generation_config = generation_config)\n",
    "    fullfinetuned_text_output = tokenizer.decode(fullfinetuned_model_outputs[0], skip_special_tokens=True)\n",
    "    fullfinetuned_model_smmaries.append(fullfinetuned_text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9e271ed-d460-415a-a35a-48d215f2b85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T16:17:03.434449Z",
     "iopub.status.busy": "2023-12-26T16:17:03.433766Z",
     "iopub.status.idle": "2023-12-26T16:17:04.448858Z",
     "shell.execute_reply": "2023-12-26T16:17:04.447771Z",
     "shell.execute_reply.started": "2023-12-26T16:17:03.434412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.22947279572730211, 'rouge2': 0.0781999252930429, 'rougeL': 0.1977396569141013, 'rougeLsum': 0.19679439493636844}\n",
      "Finetuned MODEL:\n",
      "{'rouge1': 0.4702793069042287, 'rouge2': 0.21222551380111027, 'rougeL': 0.3730673327044676, 'rougeLsum': 0.3731581482211145}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.4612038420317723, 'rouge2': 0.20200270894870215, 'rougeL': 0.37124483104115225, 'rougeLsum': 0.370299949862882}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "human_baseline_summaries = test_summaries[:150]\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "fullfinetuned_model_results = rouge.compute(\n",
    "    predictions=fullfinetuned_model_smmaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('Finetuned MODEL:')\n",
    "print(fullfinetuned_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)\n",
    "run.summary[\"rouge_score\"] = peft_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3a1ec81-f117-4bf0-a43e-9e2e38705246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T16:17:25.237883Z",
     "iopub.status.busy": "2023-12-26T16:17:25.237269Z",
     "iopub.status.idle": "2023-12-26T16:17:30.461862Z",
     "shell.execute_reply": "2023-12-26T16:17:30.461331Z",
     "shell.execute_reply.started": "2023-12-26T16:17:25.237861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'bleu': 0.05809275269853388, 'precisions': [0.2793687901811806, 0.11613691931540342, 0.061178731582319026, 0.0211978465679677], 'brevity_penalty': 0.721293229102033, 'length_ratio': 0.7537444933920705, 'translation_length': 3422, 'reference_length': 4540}\n",
      "Finetuned MODEL:\n",
      "{'bleu': 0.20580074626413755, 'precisions': [0.4581073989854819, 0.25902640560445483, 0.16485139376038396, 0.09170305676855896], 'brevity_penalty': 1.0, 'length_ratio': 1.259251101321586, 'translation_length': 5717, 'reference_length': 4540}\n",
      "PEFT MODEL:\n",
      "{'bleu': 0.2036505381322652, 'precisions': [0.4776618775831529, 0.2683025755424863, 0.16356410792721188, 0.08205571150939323], 'brevity_penalty': 1.0, 'length_ratio': 1.1191629955947135, 'translation_length': 5081, 'reference_length': 4540}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training time (seconds)</td><td>â–</td></tr><tr><td>eval/loss</td><td>â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–â–ˆâ–‡â–†â–†â–‚â–‚â–ƒâ–„â–…</td></tr><tr><td>eval/samples_per_second</td><td>â–ˆâ–â–‚â–ƒâ–ƒâ–‡â–‡â–†â–…â–„</td></tr><tr><td>eval/steps_per_second</td><td>â–ˆâ–â–‚â–ƒâ–ƒâ–‡â–†â–†â–…â–ƒ</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/learning_rate</td><td>â–„â–‡â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–ƒâ–‚â–â–‚â–â–â–â–â–</td></tr><tr><td>train/total_flos</td><td>â–</td></tr><tr><td>train/train_loss</td><td>â–</td></tr><tr><td>train/train_runtime</td><td>â–</td></tr><tr><td>train/train_samples_per_second</td><td>â–</td></tr><tr><td>train/train_steps_per_second</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training time (seconds)</td><td>1500.25268</td></tr><tr><td>eval/loss</td><td>1.15197</td></tr><tr><td>eval/runtime</td><td>40.8829</td></tr><tr><td>eval/samples_per_second</td><td>35.369</td></tr><tr><td>eval/steps_per_second</td><td>4.427</td></tr><tr><td>train/epoch</td><td>1.38</td></tr><tr><td>train/global_step</td><td>1000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.2857</td></tr><tr><td>train/total_flos</td><td>1.1130063814656e+16</td></tr><tr><td>train/train_loss</td><td>1.40755</td></tr><tr><td>train/train_runtime</td><td>1499.9424</td></tr><tr><td>train/train_samples_per_second</td><td>10.667</td></tr><tr><td>train/train_steps_per_second</td><td>0.667</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">flant5-PEFT-1703600695</strong>: <a href=\"https://wandb.ai/aambekar234/genai-llm/runs/v3u0q5xn\" target=\"_blank\">https://wandb.ai/aambekar234/genai-llm/runs/v3u0q5xn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231226_142533-v3u0q5xn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "    \n",
    "original_model_results = bleu.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries\n",
    ")\n",
    "\n",
    "fullfinetuned_model_results = bleu.compute(\n",
    "    predictions=fullfinetuned_model_smmaries,\n",
    "    references=human_baseline_summaries\n",
    ")\n",
    "\n",
    "peft_model_results = bleu.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('Finetuned MODEL:')\n",
    "print(fullfinetuned_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)\n",
    "\n",
    "run.summary[\"bleu_score\"] = peft_model_results\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586318e2-1715-4520-94ea-5a5b8f52a98f",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "With this experiment with validated the efficiency of PEFT and its advantages over full-finetuning of LLMs. Next Article we will explore RLHF techniques for further improving LLMs with help of human feedback. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f23d95-5f63-4db5-9f51-b4fe9589cb6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T16:42:09.648514Z",
     "iopub.status.busy": "2023-12-26T16:42:09.648235Z",
     "iopub.status.idle": "2023-12-26T16:42:09.653329Z",
     "shell.execute_reply": "2023-12-26T16:42:09.652590Z",
     "shell.execute_reply.started": "2023-12-26T16:42:09.648494Z"
    }
   },
   "source": [
    "**ğŸŒŸ Connect on LinkedIn!** \n",
    "\n",
    "If you've found this content _useful_ and would like to explore more about **data science**, **machine learning**, and related fields, I'd be delighted to see you on my LinkedIn network. I share insights, resources, and the latest trends that could be beneficial for your learning journey.\n",
    "\n",
    "â¤ [**_Follow on LinkedIn_**](https://www.linkedin.com/in/aambekar234/)\n",
    "\n",
    "_Your support and interaction are always appreciated._\n",
    "\n",
    "**Best Regards,**\n",
    "**Abhijeet Ambekar**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
